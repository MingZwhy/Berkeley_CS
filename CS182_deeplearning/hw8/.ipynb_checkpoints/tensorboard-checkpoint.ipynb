{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jxOnLFTwB7fV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-10-17 21:26:48--  https://raw.githubusercontent.com/Berkeley-CS182/cs182fa23_public/main/q_wandbai/architectures.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1618 (1.6K) [text/plain]\n",
      "Saving to: 'architectures.py'\n",
      "\n",
      "     0K .                                                     100%  744K=0.002s\n",
      "\n",
      "2023-10-17 21:26:48 (744 KB/s) - 'architectures.py' saved [1618/1618]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 14.8 MB/s eta 0:00:00\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from wandb) (63.4.1)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp37-cp37m-win_amd64.whl (11 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.38-py3-none-any.whl (190 kB)\n",
      "     ---------------------------------------- 190.6/190.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: PyYAML in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from wandb) (5.9.3)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n",
      "     ------------------------------------- 241.0/241.0 kB 15.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (5.0.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.7/62.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda\\anaconda_setup\\envs\\malning\\lib\\site-packages (from importlib-metadata->Click!=8.0.0,>=7.1->wandb) (3.10.0)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8792 sha256=3a43b3d11799db09ec92120b06a6c8f6deccae8ba0055f23c6f991ae93b4b1cf\n",
      "  Stored in directory: c:\\users\\cyt\\appdata\\local\\pip\\cache\\wheels\\3e\\31\\09\\fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.38 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Berkeley-CS182/cs182fa23_public/main/q_wandbai/architectures.py\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:53:56.175976Z",
     "start_time": "2023-10-18T20:53:54.606147Z"
    },
    "id": "_jLtGXflB7fY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from architectures import BasicConvNet, ResNet18, MLP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qa9yK32B7fY"
   },
   "source": [
    "# Exploring Tensorboard\n",
    "Tensorboard is a local tool for visualizing images, metrics, histograms, and more. It is designed for tensorflow, but can be integrated with torch. Let's explore tensorboard usage with an example:\n",
    "\n",
    "```\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# To start a run, call the following\n",
    "writer = SummaryWriter(comment=f'Name_of_Run')\n",
    "\n",
    "# When you want to log a value, use the writer. When adding a scalar, the format is as follows:\n",
    "# add_scalar(tag, scalar_value, global_step=None, walltime=None, new_style=False, double_precision=False)\n",
    "writer.add_scalar('Training Loss', loss.item(), step)\n",
    "\n",
    "# Finally, when you are done logging values, close the writer\n",
    "writer.close()\n",
    "```\n",
    "There are many other functionalities and methods that you are free to explore, but will not be mentioned in this notebook.\n",
    "\n",
    "## Your Task\n",
    "We will be once again building classifiers for the CIFAR-10. There are various architectures set up for you to use in the architectures.py file. Using tensorboard, please search through 5 different hyperparameter configurations. Examples of choices include: learning rate, batch size, architecture, optimization algorithm, etc. Please submit the generated plots on your pdf and answer question A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:53:59.304543Z",
     "start_time": "2023-10-18T20:53:57.952506Z"
    },
    "id": "LdVmg7rkB7fZ"
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:02.016834Z",
     "start_time": "2023-10-18T20:53:59.412142Z"
    },
    "id": "64mWSniHB7fZ",
    "outputId": "bafc92c2-6ddb-47b2-e051-a0dc912c2021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./../cifar-10/', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./../cifar-10/', train=False,\n",
    "                                       download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:02.191475Z",
     "start_time": "2023-10-18T20:54:02.175578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:03.155836Z",
     "start_time": "2023-10-18T20:54:03.142434Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_optimizer(params, optim_type, lr):\n",
    "    if optim_type == \"sgd\":\n",
    "        optimizer = optim.SGD(params, lr=lr)\n",
    "    elif optim_type == \"adam\":\n",
    "        optimizer = optim.Adam(params, lr=lr)\n",
    "    else:\n",
    "        raise ValueError(optim_type)\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def get_model(model_type):\n",
    "    if model_type == \"basicconvnet\":\n",
    "        model = BasicConvNet()\n",
    "    elif model_type == \"resnet18\":\n",
    "        model = ResNet18()\n",
    "    elif model_type == \"mlp\":\n",
    "        model = MLP()\n",
    "    else:\n",
    "        raise ValueError(model_type)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_criterion(loss_type):\n",
    "    if(loss_type == \"mse\"):\n",
    "        criterion = nn.MSELoss()\n",
    "    elif(loss_type == \"cross\"):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        raise ValueError(loss_type)\n",
    "        \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:54:04.221320Z",
     "start_time": "2023-10-18T20:54:04.201637Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(writer, dataloader, model, loss_fn, optimizer, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batch = len(dataloader)\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        if(batch % 100 == 0):\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d} / {size:>5d}]\")\n",
    "            \n",
    "    avg_loss = total_loss / num_batch\n",
    "    correct /= size\n",
    "    \n",
    "    # write into tensorboard\n",
    "    writer.add_scalar(\"Train Loss\", avg_loss, epoch)\n",
    "    writer.add_scalar(\"Train Acc\", correct, epoch)\n",
    "    \n",
    "    print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")\n",
    "            \n",
    "\n",
    "def test(writer, dataloader, model, loss_fn, epoch):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0.1\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    # write into tensorboard\n",
    "    writer.add_scalar('Test Loss', test_loss, epoch)\n",
    "    writer.add_scalar('Test Acc', correct, epoch)\n",
    "    \n",
    "    print(f\"Evaluation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T21:05:20.678308Z",
     "start_time": "2023-10-18T21:05:20.663570Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_training(trainset, testset, hyperparameters, log_dir = \"logs\"):\n",
    "    \n",
    "    print(\"---------------------------config-----------------------------\")\n",
    "    print(hyperparameters)\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    \n",
    "    name = \"\"\n",
    "    for i, key in enumerate(hyperparameters.keys()):\n",
    "        value = hyperparameters[key]\n",
    "        if i != (len(hyperparameters.keys()) - 1):\n",
    "            item = key + \"_\" + str(value) + \"_\"\n",
    "        else:\n",
    "            item = key + \"_\" + str(value)\n",
    "        name = name + item\n",
    "        \n",
    "    model_type = hyperparameters['model']\n",
    "    model = get_model(model_type)\n",
    "    loss_type = hyperparameters['loss_fn']\n",
    "    criterion = get_criterion(loss_type)\n",
    "    learning_rate = hyperparameters['lr']\n",
    "    optim_type = hyperparameters['optimizer']\n",
    "    optimizer = get_optimizer(model.parameters(), optim_type, lr=learning_rate)\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    num_epochs = hyperparameters['epochs']\n",
    "\n",
    "    # build train data loader\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    # build test data loader\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # create a tensorboard writer\n",
    "    path = log_dir + \"/\" + name\n",
    "    writer = SummaryWriter(path)\n",
    "    print(f\"log will be written to {path}\")\n",
    "    \n",
    "    model.cuda()\n",
    "    \n",
    "    for t in range(num_epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(writer, trainloader, model, criterion, optimizer, t+1)\n",
    "        test(writer, testloader, model, criterion, t+1)\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T21:05:23.589168Z",
     "start_time": "2023-10-18T21:05:23.581307Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameters1 = {\n",
    "    \"model\" : \"basicconvnet\",\n",
    "    \"lr\" : 0.0001,\n",
    "    \"loss_fn\" : \"cross\",\n",
    "    \"optimizer\" : \"adam\",\n",
    "    \"epochs\" : 5,\n",
    "    \"batch_size\" : 16\n",
    "}\n",
    "\n",
    "hyperparameters2 = {\n",
    "    \"model\" : \"resnet18\",\n",
    "    \"lr\" : 0.0001,\n",
    "    \"loss_fn\" : \"cross\",\n",
    "    \"optimizer\" : \"adam\",\n",
    "    \"epochs\" : 5,\n",
    "    \"batch_size\" : 16\n",
    "}\n",
    "\n",
    "hyperparameters3 = {\n",
    "    \"model\" : \"mlp\",\n",
    "    \"lr\" : 0.0001,\n",
    "    \"loss_fn\" : \"cross\",\n",
    "    \"optimizer\" : \"adam\",\n",
    "    \"epochs\" : 5,\n",
    "    \"batch_size\" : 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T21:05:45.271998Z",
     "start_time": "2023-10-18T21:05:45.265865Z"
    },
    "id": "5wV2g0m4B7fa"
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    # Perhaps you want to make a function to train on a certain set of hyperparameters\n",
    "    # Don't forget to use tensorboard\n",
    "    run_training(trainset, testset, hyperparameters1)\n",
    "    run_training(trainset, testset, hyperparameters2)\n",
    "    run_training(trainset, testset, hyperparameters3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "malning",
   "language": "python",
   "name": "malning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
