{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F17RDZLCCKk"
      },
      "source": [
        "# Exploring Tooling with Weights and Biases\n",
        "Similar to tensorboard, weights and biases is an application that tracks all your training metrics, and performs visualizations for you. This tool allows you to cleanly sort, organize, and visualize your experiments. In this notebook, we will go through an example of how to use wandb.ai and have you practice.\n",
        "\n",
        "1. Make an account at https://wandb.ai/site\n",
        "\n",
        "2. pip install wandb\n",
        "\n",
        "3. wandb login\n",
        "\n",
        "4. After step 3, please paste your wandb API key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUcboEwmCCKn"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/Berkeley-CS182/cs182fa23_public/main/q_wandbai/architectures.py\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Iq2_AxWlCCKo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "from architectures import BasicConvNet, ResNet18, MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First try the example provided by wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmingzwhy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>f:\\new_gitee_code\\berkeley_class\\Deep_Learning\\hw8\\wandb\\run-20231018_144542-ncz8rcp1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mingzwhy/my-awesome-project/runs/ncz8rcp1' target=\"_blank\">woven-grass-1</a></strong> to <a href='https://wandb.ai/mingzwhy/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mingzwhy/my-awesome-project' target=\"_blank\">https://wandb.ai/mingzwhy/my-awesome-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mingzwhy/my-awesome-project/runs/ncz8rcp1' target=\"_blank\">https://wandb.ai/mingzwhy/my-awesome-project/runs/ncz8rcp1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2ab9cf995764accb4731d6a0d41329e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.115196…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▄▆▆▆▇█▇</td></tr><tr><td>loss</td><td>█▅▅▂▃▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.82327</td></tr><tr><td>loss</td><td>0.09432</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">woven-grass-1</strong> at: <a href='https://wandb.ai/mingzwhy/my-awesome-project/runs/ncz8rcp1' target=\"_blank\">https://wandb.ai/mingzwhy/my-awesome-project/runs/ncz8rcp1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20231018_144542-ncz8rcp1\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "import random\n",
        "\n",
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"my-awesome-project\",\n",
        "    \n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 0.02,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"CIFAR-100\",\n",
        "    \"epochs\": 10,\n",
        "    }\n",
        ")\n",
        "\n",
        "# simulate training\n",
        "epochs = 10\n",
        "offset = random.random() / 5\n",
        "for epoch in range(2, epochs):\n",
        "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
        "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
        "    \n",
        "    # log metrics to wandb\n",
        "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
        "    \n",
        "# [optional] finish the wandb run, necessary in notebooks\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTtcUHXICCKo"
      },
      "source": [
        "## Organizing wandb Projects\n",
        "\n",
        "With each run, you will want to have a set of parameters associated with it. For example, I want to be able to log different hyperparameters that I am using, so let's clearly list them below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y1xzV--oCCKo"
      },
      "outputs": [],
      "source": [
        "project = 'CS182 WANDB.AI Practice Notebok'\n",
        "learning_rate = 0.01\n",
        "epochs = 2\n",
        "architecture ='CNN'\n",
        "dataset = 'CIFAR-10'\n",
        "batch_size = 64\n",
        "momentum = 0.9\n",
        "log_freq = 20\n",
        "print_freq = 200\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNhkpuVJCCKo"
      },
      "source": [
        "### Initializing the Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ELtM17QzCCKp"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>f:\\new_gitee_code\\berkeley_class\\Deep_Learning\\hw8\\wandb\\run-20231018_145146-erzbsuxc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mingzwhy/CS182%20WANDB.AI%20Practice%20Notebok/runs/erzbsuxc' target=\"_blank\">pretty-universe-1</a></strong> to <a href='https://wandb.ai/mingzwhy/CS182%20WANDB.AI%20Practice%20Notebok' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mingzwhy/CS182%20WANDB.AI%20Practice%20Notebok' target=\"_blank\">https://wandb.ai/mingzwhy/CS182%20WANDB.AI%20Practice%20Notebok</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mingzwhy/CS182%20WANDB.AI%20Practice%20Notebok/runs/erzbsuxc' target=\"_blank\">https://wandb.ai/mingzwhy/CS182%20WANDB.AI%20Practice%20Notebok/runs/erzbsuxc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mingzwhy/CS182%20WANDB.AI%20Practice%20Notebok/runs/erzbsuxc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x15514048828>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=project,\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"architecture\": architecture,\n",
        "    \"dataset\": dataset,\n",
        "    \"epochs\": epochs,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"momentum\": momentum\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGMWgtruCCKp"
      },
      "source": [
        "From here on, we have some standard CIFAR training definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Yc3SmQ4uCCKp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./../cifar-10/', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./../cifar-10/', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NUXVG5u_CCKp"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xNWkTyJJCCKp"
      },
      "outputs": [],
      "source": [
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OXiCmqXHCCKq"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2hENC_1CCKq"
      },
      "source": [
        "### Training with wandb\n",
        "\n",
        "As you can see, similar to tensorboard, each gradient step we will want to log the accuracy and loss. See below for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vz9NckDXCCKq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 2.25610 accuracy: 15.78125\n",
            "[1,   400] loss: 1.90479 accuracy: 29.90625\n",
            "[1,   600] loss: 1.69474 accuracy: 37.71875\n",
            "[2,   200] loss: 1.48649 accuracy: 45.19531\n",
            "[2,   400] loss: 1.43179 accuracy: 48.33594\n",
            "[2,   600] loss: 1.38526 accuracy: 50.20312\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        accuracy = torch.mean((torch.argmax(outputs, dim=1) == labels).float()).item() * 100\n",
        "\n",
        "        # print statistics\n",
        "        running_acc += accuracy\n",
        "        running_loss += loss.item()\n",
        "        if i % log_freq == log_freq - 1:\n",
        "            wandb.log({'accuracy': accuracy, 'loss': loss.item()})\n",
        "\n",
        "        if i % print_freq == print_freq - 1:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_freq:.5f} accuracy: {running_acc/print_freq:.5f}')\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4SilioQCCKq"
      },
      "source": [
        "After we are done with this run, we will want to call\n",
        " `wandb.finish()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S4kvb5nHCCKq"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41a68fbcfd9346f69683814bd11a624e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▂▁▁▅▂▃▅▄▅▅▃▅▄▆▇▅▇▆▅▆▆███▆▇▇▇▆▇▆▆▇▇▇▇▇▆█</td></tr><tr><td>loss</td><td>████▇▇▆▅▅▆▅▅▄▄▄▂▄▄▄▃▃▂▁▂▁▃▂▃▃▃▃▃▂▁▃▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>56.25</td></tr><tr><td>loss</td><td>1.31502</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pretty-universe-1</strong> at: <a href='https://wandb.ai/mingzwhy/CS182%20WANDB.AI%20Practice%20Notebok/runs/erzbsuxc' target=\"_blank\">https://wandb.ai/mingzwhy/CS182%20WANDB.AI%20Practice%20Notebok/runs/erzbsuxc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20231018_145146-erzbsuxc\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHgdDK2kCCKq"
      },
      "source": [
        "## Your Task\n",
        "We will be once again building classifiers for the CIFAR-10. There are various architectures set up for you to use in the architectures.py file. Using wandb, please search through 10 different hyperparameter configurations. Examples of choices include: learning rate, batch size, architecture, optimization algorithm, etc. Please submit the hyperparameters that result in the highest accuracies for this classification task. Please then explore wandb for all the visualization that you may need. In addition, feel free to run as many epochs as you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFRgBpgBCCKq"
      },
      "outputs": [],
      "source": [
        "def run(params):\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD29zxJuCCKq"
      },
      "source": [
        "This software/tutorial is based on PyTorch, an open-source project available at https://github.com/pytorch/tutorials/\n",
        "\n",
        "There is a BSD 3-Clause License as seen here: https://github.com/pytorch/tutorials/blob/main/LICENSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "from architectures import BasicConvNet, ResNet18, MLP\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./../cifar-10/', train=True,\n",
        "                                    download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./../cifar-10/', train=False,\n",
        "                                       download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_optimizer(params, optim_type, lr):\n",
        "    if optim_type == \"sgd\":\n",
        "        optimizer = optim.SGD(params, lr=lr)\n",
        "    elif optim_type == \"adam\":\n",
        "        optimizer = optim.Adam(params, lr=lr)\n",
        "    else:\n",
        "        raise ValueError(optim_type)\n",
        "    \n",
        "    return optimizer\n",
        "\n",
        "def get_model(model_type):\n",
        "    if model_type == \"basicconvnet\":\n",
        "        model = BasicConvNet()\n",
        "    elif model_type == \"resnet18\":\n",
        "        model = ResNet18()\n",
        "    elif model_type == \"mlp\":\n",
        "        model = MLP()\n",
        "    else:\n",
        "        raise ValueError(model_type)\n",
        "    \n",
        "    return model\n",
        "\n",
        "def get_criterion(loss_type):\n",
        "    if(loss_type == \"mse\"):\n",
        "        criterion = nn.MSELoss()\n",
        "    elif(loss_type == \"cross\"):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        raise ValueError(loss_type)\n",
        "        \n",
        "    return criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, epoch):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batch = len(dataloader)\n",
        "    model.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        \n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "        \n",
        "        if(batch % 100 == 0):\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f} [{current:>5d} / {size:>5d}]\")\n",
        "            \n",
        "    avg_loss = total_loss / num_batch\n",
        "    correct /= size\n",
        "    \n",
        "    # write into wandb\n",
        "    wandb.log({'train accuracy': correct, 'train loss': avg_loss})\n",
        "    \n",
        "    print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")\n",
        "            \n",
        "\n",
        "def test(dataloader, model, loss_fn, epoch):\n",
        "    \n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0.1\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.cuda(), y.cuda()\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    \n",
        "    # write into wandb\n",
        "    wandb.log({'test accuracy': correct, 'test loss': test_loss})\n",
        "    \n",
        "    print(f\"Evaluation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training(trainset, testset, hyperparameters, log_dir = \"logs\"):\n",
        "    \n",
        "    print(\"---------------------------config-----------------------------\")\n",
        "    print(hyperparameters)\n",
        "    print(\"--------------------------------------------------------------\")\n",
        "    \n",
        "    name = \"\"\n",
        "    for i, key in enumerate(hyperparameters.keys()):\n",
        "        value = hyperparameters[key]\n",
        "        if i != (len(hyperparameters.keys()) - 1):\n",
        "            item = key + \"_\" + str(value) + \"_\"\n",
        "        else:\n",
        "            item = key + \"_\" + str(value)\n",
        "        name = name + item\n",
        "        \n",
        "    model_type = hyperparameters['model']\n",
        "    model = get_model(model_type)\n",
        "    loss_type = hyperparameters['loss_fn']\n",
        "    criterion = get_criterion(loss_type)\n",
        "    learning_rate = hyperparameters['lr']\n",
        "    optim_type = hyperparameters['optimizer']\n",
        "    optimizer = get_optimizer(model.parameters(), optim_type, lr=learning_rate)\n",
        "    batch_size = hyperparameters['batch_size']\n",
        "    num_epochs = hyperparameters['epochs']\n",
        "\n",
        "    # build train data loader\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    # build test data loader\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    # create a wandb project\n",
        "\n",
        "    wandb.init(\n",
        "        # set the wandb project where this run will be logged\n",
        "        project = name,\n",
        "\n",
        "        # track hyperparameters and run metadata\n",
        "        config={\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"architecture\": model_type,\n",
        "        \"dataset\": 'CIFAR-10',\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    print(f\"log will be written to project {name}\")\n",
        "    \n",
        "    model.cuda()\n",
        "    \n",
        "    for t in range(num_epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        train(trainloader, model, criterion, optimizer, t+1)\n",
        "        test(testloader, model, criterion, t+1)\n",
        "        \n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51L-SEg_CCKq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparameters1 = {\n",
        "    \"model\" : \"basicconvnet\",\n",
        "    \"lr\" : 0.0001,\n",
        "    \"loss_fn\" : \"cross\",\n",
        "    \"optimizer\" : \"adam\",\n",
        "    \"epochs\" : 3,\n",
        "    \"batch_size\" : 16\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------config-----------------------------\n",
            "{'model': 'basicconvnet', 'lr': 0.0001, 'loss_fn': 'cross', 'optimizer': 'adam', 'epochs': 3, 'batch_size': 16}\n",
            "--------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>f:\\new_gitee_code\\berkeley_class\\Deep_Learning\\hw8\\wandb\\run-20231018_150815-781t4xy1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mingzwhy/model_basicconvnet_lr_0.0001_loss_fn_cross_optimizer_adam_epochs_3_batch_size_16/runs/781t4xy1' target=\"_blank\">swept-cloud-1</a></strong> to <a href='https://wandb.ai/mingzwhy/model_basicconvnet_lr_0.0001_loss_fn_cross_optimizer_adam_epochs_3_batch_size_16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mingzwhy/model_basicconvnet_lr_0.0001_loss_fn_cross_optimizer_adam_epochs_3_batch_size_16' target=\"_blank\">https://wandb.ai/mingzwhy/model_basicconvnet_lr_0.0001_loss_fn_cross_optimizer_adam_epochs_3_batch_size_16</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mingzwhy/model_basicconvnet_lr_0.0001_loss_fn_cross_optimizer_adam_epochs_3_batch_size_16/runs/781t4xy1' target=\"_blank\">https://wandb.ai/mingzwhy/model_basicconvnet_lr_0.0001_loss_fn_cross_optimizer_adam_epochs_3_batch_size_16/runs/781t4xy1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "log will be written to project model_basicconvnet_lr_0.0001_loss_fn_cross_optimizer_adam_epochs_3_batch_size_16\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.275403 [    0 / 50000]\n",
            "loss: 2.295876 [ 1600 / 50000]\n",
            "loss: 2.296754 [ 3200 / 50000]\n",
            "loss: 2.475876 [ 4800 / 50000]\n",
            "loss: 2.034561 [ 6400 / 50000]\n",
            "loss: 2.102191 [ 8000 / 50000]\n",
            "loss: 1.770932 [ 9600 / 50000]\n",
            "loss: 1.761662 [11200 / 50000]\n",
            "loss: 2.007025 [12800 / 50000]\n",
            "loss: 1.983951 [14400 / 50000]\n",
            "loss: 2.170810 [16000 / 50000]\n",
            "loss: 1.969955 [17600 / 50000]\n",
            "loss: 2.043669 [19200 / 50000]\n",
            "loss: 1.583350 [20800 / 50000]\n",
            "loss: 1.858032 [22400 / 50000]\n",
            "loss: 1.813352 [24000 / 50000]\n",
            "loss: 2.695940 [25600 / 50000]\n",
            "loss: 1.553071 [27200 / 50000]\n",
            "loss: 2.006852 [28800 / 50000]\n",
            "loss: 1.640339 [30400 / 50000]\n",
            "loss: 1.715128 [32000 / 50000]\n",
            "loss: 1.305841 [33600 / 50000]\n",
            "loss: 1.505121 [35200 / 50000]\n",
            "loss: 1.876178 [36800 / 50000]\n",
            "loss: 1.848950 [38400 / 50000]\n",
            "loss: 1.486462 [40000 / 50000]\n",
            "loss: 1.894930 [41600 / 50000]\n",
            "loss: 1.758148 [43200 / 50000]\n",
            "loss: 1.637692 [44800 / 50000]\n",
            "loss: 1.428107 [46400 / 50000]\n",
            "loss: 1.890333 [48000 / 50000]\n",
            "loss: 2.021170 [49600 / 50000]\n",
            "Train Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.873440 \n",
            "\n",
            "Evaluation Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.695036 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.945902 [    0 / 50000]\n",
            "loss: 1.961833 [ 1600 / 50000]\n",
            "loss: 2.045042 [ 3200 / 50000]\n",
            "loss: 1.767606 [ 4800 / 50000]\n",
            "loss: 1.843510 [ 6400 / 50000]\n",
            "loss: 1.653365 [ 8000 / 50000]\n",
            "loss: 1.659791 [ 9600 / 50000]\n",
            "loss: 1.573831 [11200 / 50000]\n",
            "loss: 1.824797 [12800 / 50000]\n",
            "loss: 1.739236 [14400 / 50000]\n",
            "loss: 1.682178 [16000 / 50000]\n",
            "loss: 1.321580 [17600 / 50000]\n",
            "loss: 1.650610 [19200 / 50000]\n",
            "loss: 1.652206 [20800 / 50000]\n",
            "loss: 1.767516 [22400 / 50000]\n",
            "loss: 1.838782 [24000 / 50000]\n",
            "loss: 1.335537 [25600 / 50000]\n",
            "loss: 1.327205 [27200 / 50000]\n",
            "loss: 1.498140 [28800 / 50000]\n",
            "loss: 1.675665 [30400 / 50000]\n",
            "loss: 1.503295 [32000 / 50000]\n",
            "loss: 1.458065 [33600 / 50000]\n",
            "loss: 1.367257 [35200 / 50000]\n",
            "loss: 1.135213 [36800 / 50000]\n",
            "loss: 1.445645 [38400 / 50000]\n",
            "loss: 1.701791 [40000 / 50000]\n",
            "loss: 1.640828 [41600 / 50000]\n",
            "loss: 1.712201 [43200 / 50000]\n",
            "loss: 1.537874 [44800 / 50000]\n",
            "loss: 1.905664 [46400 / 50000]\n",
            "loss: 1.124830 [48000 / 50000]\n",
            "loss: 1.459387 [49600 / 50000]\n",
            "Train Error: \n",
            " Accuracy: 41.3%, Avg loss: 1.621976 \n",
            "\n",
            "Evaluation Error: \n",
            " Accuracy: 43.9%, Avg loss: 1.566050 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.535134 [    0 / 50000]\n",
            "loss: 1.805351 [ 1600 / 50000]\n",
            "loss: 1.786749 [ 3200 / 50000]\n",
            "loss: 1.310191 [ 4800 / 50000]\n",
            "loss: 1.752694 [ 6400 / 50000]\n",
            "loss: 1.204131 [ 8000 / 50000]\n",
            "loss: 1.598748 [ 9600 / 50000]\n",
            "loss: 2.172778 [11200 / 50000]\n",
            "loss: 1.727211 [12800 / 50000]\n",
            "loss: 1.796473 [14400 / 50000]\n",
            "loss: 1.411669 [16000 / 50000]\n",
            "loss: 1.535793 [17600 / 50000]\n",
            "loss: 1.546669 [19200 / 50000]\n",
            "loss: 1.406159 [20800 / 50000]\n",
            "loss: 1.834090 [22400 / 50000]\n",
            "loss: 1.424159 [24000 / 50000]\n",
            "loss: 1.532046 [25600 / 50000]\n",
            "loss: 1.406389 [27200 / 50000]\n",
            "loss: 1.402709 [28800 / 50000]\n",
            "loss: 1.617857 [30400 / 50000]\n",
            "loss: 1.279262 [32000 / 50000]\n",
            "loss: 1.512915 [33600 / 50000]\n",
            "loss: 1.019401 [35200 / 50000]\n",
            "loss: 2.139595 [36800 / 50000]\n",
            "loss: 2.018800 [38400 / 50000]\n",
            "loss: 1.252915 [40000 / 50000]\n",
            "loss: 1.524170 [41600 / 50000]\n",
            "loss: 1.198219 [43200 / 50000]\n",
            "loss: 1.580286 [44800 / 50000]\n",
            "loss: 1.341067 [46400 / 50000]\n",
            "loss: 1.791046 [48000 / 50000]\n",
            "loss: 1.316368 [49600 / 50000]\n",
            "Train Error: \n",
            " Accuracy: 44.7%, Avg loss: 1.525798 \n",
            "\n",
            "Evaluation Error: \n",
            " Accuracy: 47.0%, Avg loss: 1.479400 \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c3e8f9af4294ce8952a2f14f56310ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test accuracy</td><td>▁▆█</td></tr><tr><td>test loss</td><td>█▄▁</td></tr><tr><td>train accuracy</td><td>▁▆█</td></tr><tr><td>train loss</td><td>█▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test accuracy</td><td>0.46991</td></tr><tr><td>test loss</td><td>1.4794</td></tr><tr><td>train accuracy</td><td>0.44718</td></tr><tr><td>train loss</td><td>1.5258</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">swept-cloud-1</strong> at: <a href='https://wandb.ai/mingzwhy/model_basicconvnet_lr_0.0001_loss_fn_cross_optimizer_adam_epochs_3_batch_size_16/runs/781t4xy1' target=\"_blank\">https://wandb.ai/mingzwhy/model_basicconvnet_lr_0.0001_loss_fn_cross_optimizer_adam_epochs_3_batch_size_16/runs/781t4xy1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20231018_150815-781t4xy1\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run_training(trainset, testset, hyperparameters1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "malning",
      "language": "python",
      "name": "malning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
