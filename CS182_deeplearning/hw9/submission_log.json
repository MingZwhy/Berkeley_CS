{"time": 1698288023.9821537, "torch_transformer_shape": [8, 3], "torch_transformer_value": [0.051710158586502075, -0.1363382786512375, 0.12136831134557724, 0.012151258066296577, -0.2020265758037567], "torch_transformer_init": "    def __init__(self, input_dim, qk_dim, v_dim, pos_dim=None, max_seq_len=10):\n        super().__init__()\n        if pos_dim is not None:\n            self.pos = nn.Embedding(max_seq_len, pos_dim)\n        else:\n            self.pos = None\n        in_dim = input_dim\n        if pos_dim is not None:\n            in_dim += pos_dim\n        \n        ########################################################################\n        # TODO: Define query, key, value projection layers Qm, Km, Vm.\n        #       Each of them is a linear projection without bias \n        ########################################################################\n        self.Qm = nn.Linear(in_features=in_dim, out_features=qk_dim, bias=False)\n        self.Km = nn.Linear(in_features=in_dim, out_features=qk_dim, bias=False)\n        self.Vm = nn.Linear(in_features=in_dim, out_features=v_dim, bias=False)\n        ########################################################################\n        \n        self.d_k = qk_dim\n", "torch_transformer_forward": "    def forward(self, seq):\n        \"\"\"\n        Transformer forward pass\n        \n        Inputs: seq is a torch tensor of shape (seq_len, input_dim).\n        Outputs: a torch tensor of shape (seq_len, v_dim), the output of the attention operation\n        \"\"\"\n        ################################################################################################\n        # TODO: Implement the forward pass of the `PytorchTransformer` class.\n        #       The forward pass should be identical to the forward pass of the\n        #       `NumpyTransformer` class.\n        # \n        # Hint: The attention operation should be implemented as\n        #       If `pos` exists, it should be concatenated to the input sequence.\n        #################################################################################################\n        if self.pos is not None:\n            seq_len = seq.shape[0]\n            pos_vector = self.pos.weight[:seq_len]\n            seq = torch.cat([seq, pos_vector], dim=-1)\n            \n        K = self.Km(seq)    # seq_len x qk_dim\n        Q = self.Qm(seq)    # seq_len x qk_dim\n        V = self.Vm(seq)    # seq_len x v_dim\n        \n        # Compute attention\n        outputs = []\n        \n        for i, q in enumerate(Q):\n            #dot = K @ q\n            dot = torch.matmul(K, q)\n            #dot = dot / np.sqrt(self.qk_dim)\n            dot = dot / np.sqrt(self.d_k)\n            #softmax_dot = np.exp(dot) / np.sum(np.exp(dot), axis=-1, keepdims=True)\n            #dot : (seq_len,)\n            softmax_dot = nn.functional.softmax(dot, dim=-1)\n            #out_i = softmax_dot @ V\n            #out_i (v_dim,) = softmax_dot(seq_len,) @ V(seq_len, v_dim)\n            out_i = torch.matmul(softmax_dot, V)\n            outputs.append(out_i)\n            \n        out = torch.stack(outputs)\n            \n        ################################################################################################\n        # END OF YOUR CODE\n        ################################################################################################\n        return out\n", "attention_by_position": [0.9999999958776927, 4.12230721938699e-09, 0.0, 0.9999999958776927, 4.12230721938699e-09, 0.0, 0.9999999958776927, 4.12230721938699e-09, 0.0, 0.9999999958776927, 4.12230721938699e-09, 0.0], "attention_by_position_pos": [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], "attention_by_position_Q": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0], "attention_by_position_K": [0.0, 0.0, 0.0, 20.0, 0.0, 0.0, 0.0], "attention_by_position_V": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "attention_by_content": [7.812492933054883e-12, 2.6041643110182943e-12, 0.9999999999895833, 0.9999999999947916, 1.7361095406879051e-12, 3.4722190813758102e-12, 0.9999999999947916, 1.7361095406879051e-12, 3.4722190813758102e-12, 7.812492933054883e-12, 2.6041643110182943e-12, 0.9999999999895833, 1.5624985865865626e-11, 0.9999999999739584, 1.0416657243910416e-11, 0.9999999999947916, 1.7361095406879051e-12, 3.4722190813758102e-12], "attention_by_content_Q": [9.0, 0.0, 0.0, 0.0, 9.0, 0.0, 0.0, 0.0, 9.0], "attention_by_content_K": [5.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 5.0], "attention_by_content_V": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}