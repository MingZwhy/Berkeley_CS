{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles of CNN\n",
    "\n",
    "Many materials are from Deep Learning book (Ian Goodfellow, Yoshua Bengio and Aaron Courville)\n",
    "\n",
    "Why does CNN perform better than MLP (Multilayer Perceptron) in various modalities? In this notebook, you will understand the principle of CNN through various examples and experiments. The three most distinct features that differentiate CNN from MLP are as follows:\n",
    "\n",
    "1. **sparse interactions** : Unlike the MLP model, which had to calculate the interactions between all neurons using matrix multiplication, CNN has sparse interactions. This is achieved by using smaller kernels in comparison to the resolution of the input image. This means that CNN can greatly reduce the amount of computation and memory requirements and improve statistical efficiency. This is also called sparse connectivity or sparse weights.\n",
    "\n",
    "\n",
    "2. **parameter sharing** : Parameter sharing means using the same parameters more than once within a model. In the case of MLP, all parameters are used only once when calculating the output within one layer. This reduces the memory used to store parameters.\n",
    "\n",
    "\n",
    "3. **translational equivariance** : Parameter sharing in convolution operation makes the convolution layer equivariant to translation of given input. When a function is equivariant to some operation, it means that when the input changes as much as the given operation, the output of the function also changes in the same way. To explain it more formally, if a function $f(x)$ is equivariant to a transformation $g(x)$, then $f(g(x)) = g(f(x))$. In the case of convolution, $g(x)$ is the translation of the input $x$. \n",
    "\n",
    "    While convolution is equivariant to translation, it is not equivariant to other transformations such as rotation, scale, or warping. Therefore, various regularizations such as data augmentations are used to obtain CNN functions that are robust to such transformations during training. However, this will not be covered in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP vs CNN\n",
    "\n",
    "In this problem, we will learn why the properties that CNN have lead to better performance on the vision modality by comparing it with MLP. \n",
    "\n",
    "The number of parameters is the rule of thumb to compare the expressiveness of the neural network. So we are now comparing MLP and CNN that have the similar number of parameters. Let's see how the performance deviates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "PyTorch provides convenient tools to automate the process of downloading \n",
    "common datasets, processing the data, and splitting into minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "# If we want to add data augmentations, torchvision also offers different \n",
    "# transformations that we can compose in here, though we would need to be sure\n",
    "# to have two sets of transformations: one with data augmentation for the \n",
    "# training loaders, and one without for the test and validation sets.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./../../cifar-10/', train=True, download=False,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./../../cifar-10/', train=True, download=False,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./../../cifar-10/', train=False, download=False, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is recommended, but not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment.\n",
    "\n",
    "## Colab Users\n",
    "\n",
    "If you are using Colab, you need to manually switch to a GPU device. You can do this by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`. Note that you have to rerun the cells from the top since the kernel gets restarted upon switching runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 500\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid_accuracy(loader, model):\n",
    "    # print('Checking accuracy on validation set')\n",
    "    if not loader.dataset.train:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        # print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc\n",
    "\n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API and prints model \n",
    "    accuracies during training.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Lists of validation accuracies at the end of each epoch.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each trainable parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                acc = check_valid_accuracy(loader_val, model)\n",
    "                print(f\"Epoch [{e}/{epochs}]: Iter {t}, loss = {round(loss.item(), 4)}, validation accuracy = {100*acc}%\")\n",
    "        val_accs.append(check_valid_accuracy(loader_val, model))\n",
    "    return val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define 3 Layer MLP and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(nn.Module):\n",
    "    def __init__(self, in_channel, channel_1, channel_2, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, channel_1, 5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(channel_1, channel_2, 3, stride=1, padding=1)\n",
    "        self.classifier = nn.Linear(channel_2 * 32 * 32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x=  F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of parameters\n",
    "\n",
    "Note that with the given architecture hyperparameters, the number of parameters of MLP model is slightly larger than that of CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params of MLP model : 1201910\n",
      "#params of CNN model : 1121710\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    return num_params\n",
    "\n",
    "input_size = 3 * 32 * 32\n",
    "in_channel = 3\n",
    "channel_1 = 100\n",
    "channel_2 = 100\n",
    "num_classes = 10\n",
    "hidden_size = 350\n",
    "\n",
    "mlp_model = ThreeLayerMLP(input_size, hidden_size, num_classes)\n",
    "cnn_model = ThreeLayerConvNet(in_channel, channel_1, channel_2, num_classes)\n",
    "\n",
    "num_params_mlp = count_parameters(mlp_model)\n",
    "num_params_cnn = count_parameters(cnn_model)\n",
    "\n",
    "print(f\"#params of MLP model : {num_params_mlp}\")\n",
    "print(f\"#params of CNN model : {num_params_cnn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Now we will train both MLP and CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 350]       1,075,550\n",
      "            Linear-2                  [-1, 350]         122,850\n",
      "            Linear-3                   [-1, 10]           3,510\n",
      "================================================================\n",
      "Total params: 1,201,910\n",
      "Trainable params: 1,201,910\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 4.58\n",
      "Estimated Total Size (MB): 4.60\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 100, 32, 32]           7,600\n",
      "            Conv2d-2          [-1, 100, 32, 32]          90,100\n",
      "            Linear-3                   [-1, 10]       1,024,010\n",
      "================================================================\n",
      "Total params: 1,121,710\n",
      "Trainable params: 1,121,710\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.56\n",
      "Params size (MB): 4.28\n",
      "Estimated Total Size (MB): 5.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "format_input_size = (3,32,32)\n",
    "\n",
    "mlp_model = mlp_model.cuda()\n",
    "cnn_model = cnn_model.cuda()\n",
    "\n",
    "summary(mlp_model, input_size = format_input_size)\n",
    "summary(cnn_model, input_size = format_input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start MLP training...\n",
      "Epoch [0/2]: Iter 0, loss = 2.3459, validation accuracy = 12.2%\n",
      "Epoch [0/2]: Iter 500, loss = 2.0613, validation accuracy = 32.300000000000004%\n",
      "Epoch [1/2]: Iter 0, loss = 1.7054, validation accuracy = 34.599999999999994%\n",
      "Epoch [1/2]: Iter 500, loss = 1.8353, validation accuracy = 38.0%\n",
      "Start CNN training...\n",
      "Epoch [0/2]: Iter 0, loss = 2.3155, validation accuracy = 11.700000000000001%\n",
      "Epoch [0/2]: Iter 500, loss = 1.5784, validation accuracy = 46.9%\n",
      "Epoch [1/2]: Iter 0, loss = 1.4729, validation accuracy = 49.2%\n",
      "Epoch [1/2]: Iter 500, loss = 1.449, validation accuracy = 53.2%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "\n",
    "mlp_optimizer = optim.SGD(mlp_model.parameters(), lr=learning_rate)\n",
    "cnn_optimizer = optim.SGD(cnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_epochs = 2\n",
    "# total_epochs = 5\n",
    "\n",
    "print(\"Start MLP training...\")\n",
    "mlp_accuracy = train_model(mlp_model, mlp_optimizer, total_epochs)\n",
    "print(\"Start CNN training...\")\n",
    "cnn_accuracy = train_model(cnn_model, cnn_optimizer, total_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What is the final accuracy of MLP and CNN models? Why are they different?\n",
    "\n",
    "A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translational Equivariance\n",
    "\n",
    "In this problem, we will check that CNN is translationally equivraint and MLP is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some helpers\n",
    "def torch_to_numpy(tensor):\n",
    "    tensor = tensor.cpu().detach().numpy()\n",
    "    return tensor\n",
    "\n",
    "def preprocess_mnist_data(data):\n",
    "    # padding tuples: (padding_left, padding_right, padding_top, padding_bottom)\n",
    "    # data1 = F.pad(data, (0, 28, 0, 28), mode='constant', value=0)\n",
    "    # data2 = F.pad(data, (28, 0, 0, 28), mode='constant', value=0)\n",
    "    # data3 = F.pad(data, (0, 28, 28, 0), mode='constant', value=0)\n",
    "    # data4 = F.pad(data, (28, 0, 28, 0), mode='constant', value=0)\n",
    "    # data = torch.cat((data1, data2, data3, data4), dim=0)\n",
    "\n",
    "    padded_data_list = []\n",
    "\n",
    "    for i in range(0, 28, 4):\n",
    "        for j in range(0, 28, 4):\n",
    "            padded_data_list.append(F.pad(data, (i, 28-i, j, 28-j), mode='constant', value=0))\n",
    "    \n",
    "    padded_data = torch.stack(padded_data_list, dim=0)\n",
    "\n",
    "    return padded_data\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 3, 1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(20, 40, 3, 1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(40, 1, 3, 1, padding=1)\n",
    "        # self.conv4 = nn.Conv2d(40, 1, 3, 1, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))   \n",
    "        x = F.relu(self.conv2(x)) \n",
    "        x = F.relu(self.conv3(x)) \n",
    "        # x = F.relu(self.conv4(x)) \n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(56*56, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 56*56)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))   \n",
    "        x = F.relu(self.fc2(x)) \n",
    "        x = F.relu(self.fc3(x)) \n",
    "        x = x.reshape((bs, 1, 56, 56))\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 1, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "mnist_train = dset.MNIST('./deeplearning/datasets', train=True, download=True)\n",
    "sample_index = 12\n",
    "mnist_sample = mnist_train[sample_index][0]\n",
    "mnist_sample = T.functional.pil_to_tensor(mnist_sample)\n",
    "\n",
    "mnist_sample = preprocess_mnist_data(mnist_sample)\n",
    "print(mnist_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the output of MLP and CNN with different translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive, widgets, Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN().to(device)\n",
    "mlp_model = MLP().to(device)\n",
    "\n",
    "mnist_sample = mnist_sample.to(device)\n",
    "# Convert to float32\n",
    "mnist_sample = mnist_sample.float()\n",
    "\n",
    "cnn_output = torch_to_numpy(cnn_model(mnist_sample))\n",
    "mlp_output = torch_to_numpy(mlp_model(mnist_sample))\n",
    "mnist_sample = torch_to_numpy(mnist_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec8de32bdd04a4093f477e8a5ffdf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=48), Output()), _dom_classes=('widget-interact',â€¦"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Main update function for interactive plot\n",
    "def update_images(i):\n",
    "    fig.clear()\n",
    "    f, axarr = plt.subplots(1,3, figsize=(15, 5))\n",
    "    \n",
    "    # Show the images\n",
    "    axarr[0].imshow(mnist_sample[i, 0, :, :])\n",
    "    axarr[1].imshow(cnn_output[i, 0, :, :])\n",
    "    axarr[2].imshow(mlp_output[i, 0, :, :])\n",
    "\n",
    "    # Set the titles\n",
    "    axarr[0].set_title('Input Image')\n",
    "    axarr[1].set_title('CNN Output')\n",
    "    axarr[2].set_title('MLP Output')\n",
    "\n",
    "    plt.axis('off')\n",
    "\n",
    "# Create interactive plot\n",
    "ip = interactive(update_images, i=widgets.IntSlider(min=0, max=48, step=1, value=0))\n",
    "ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What do you observe? Why is it different from the case of MLP?\n",
    "\n",
    "A.\n",
    "\n",
    "Q. Note that even though CNN is not trained, the feature maps not only are still translationally equivariant but also extract a quite good features. Why is it so?\n",
    "\n",
    "A.\n",
    "\n",
    "Q. Then what happened if we freeze CNN bacbone and train only the final layer? And why?\n",
    "\n",
    "A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malning",
   "language": "python",
   "name": "malning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9604dfe0e4c72ca50dc6ed9879659f643ef83f1cac17a709ea514e2e16450d20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
